{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86e4fe6",
   "metadata": {},
   "source": [
    "**CS6700: Reinforcement Learning**\n",
    "\n",
    "*Assignment 3*\n",
    "\n",
    "Team Members\n",
    "- Prajwal Shettigar J (CH22D302)\n",
    "- Akash Sharma (EE21S056)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79748bdd",
   "metadata": {},
   "source": [
    "# Description of Options\n",
    "\n",
    "The following description is for the default set of options that are used in this assignment. The implementation closely follows the formulation in [Between MDPs and semi-MDPs : A framework for temporal abstraction in reinforcement learning](https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf). \n",
    "\n",
    "1. **Primitive Options** : Actions can be considered as a special case of options which only execute for a single step, terminate after the taking the said step and have the state space itself as the initiation set. \n",
    "\n",
    "2. **Multi-step Options** : Deterministic policies for the set of temporally extended options (`Opt_R`, `Opt_G`, `Opt_Y` and `Opt_B`) are used as is the case in the illustration example in the reference mentioned above. These policies are defined in the code cell below. Each location in the `Taxi-v3` environment grid is assigned one of the four movement actions i.e., `South` , `North`, `East` and `West` corresponding to integers $0$, $1$, $2$ and $3$. These options terminate at their respective locations as described in the environment and the options execute only if they are not present at these locations.\n",
    "\n",
    "These options were chosen by *policy over options*, for this case epsilon-greedy policy was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primitive options are special cases of options which last exactly one step.\n",
    "Primitive options are South, North, East, West, Pick, Drop.\n",
    "Rest of the options defined are multi-step i.e., temporally extended.\n",
    "Temporally extended options are Opt_R, Opt_G, Opt_Y, Opt_B\n",
    "\n",
    "These options correspond to [0,1,2,3,4,5,6,7,8,9] respectively.\n",
    "'''\n",
    "\n",
    "default_options = [\"South\", \"North\", \"East\", \"West\", \"Pick\", \"Drop\", \n",
    "           \"Opt_R\", \"Opt_G\", \"Opt_Y\", \"Opt_B\"]\n",
    "\n",
    "# 0: South, 1: North, 2: East, 3: West\n",
    "# Here, the policy is defined as (x,y) location-action pair, this will be translated to state-action pair for options\n",
    "# Deterministic policy which enables the agent to get goal R\n",
    "\n",
    "Opt_R_policy = np.array([[1,3,0,0,0],\n",
    "                         [1,3,0,0,0],\n",
    "                         [1,3,3,3,3],\n",
    "                         [1,1,1,1,1],\n",
    "                         [1,1,1,1,1]])\n",
    "\n",
    "# Deterministic policy which enables the agent to get goal G\n",
    "Opt_G_policy = np.array([[0,0,2,2,1],\n",
    "                         [0,0,2,2,1],\n",
    "                         [2,2,2,1,1],\n",
    "                         [1,2,1,1,1],\n",
    "                         [1,2,1,1,1]])\n",
    "\n",
    "# Deterministic policy which enables the agent to get goal Y\n",
    "Opt_Y_policy = np.array([[0,3,0,0,0],\n",
    "                         [0,3,0,0,0],\n",
    "                         [0,3,3,3,3],\n",
    "                         [0,1,1,1,3],\n",
    "                         [0,1,3,1,3]])\n",
    "\n",
    "# Deterministic policy which enables the agent to get goal B\n",
    "Opt_B_policy = np.array([[0,0,0,0,3],\n",
    "                         [0,0,0,0,3],\n",
    "                         [2,2,2,0,3],\n",
    "                         [1,1,1,0,3],\n",
    "                         [1,1,1,0,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50f116",
   "metadata": {},
   "source": [
    "# SMDP Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc7d4baa",
   "metadata": {},
   "source": [
    "In SMDP Q-Learning, all the options were used as in `default_options` shown in the code snippet above. The Q-value update equation for temporally extended option in SMDP Q-Learning needs to calculated where a series of actions is taken according to the policy of that particular option. If an option is chosen by policy over options is non-executable, then there is no change in the state and no reward is incurred. A code snippet from the function `smdp_ql_taxi` is shown below where the Q-value update for a multistep option is shown. The `multistep_opt` function is also shown which takes care of the calculation of the cumulative reward and moves the taxi according the option's deterministic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE SNIPPET FROM smdp_ql_taxi for Q-value update of multi-step option ###\n",
    "\n",
    "# Check if state is part of initiation set and decide if the option has to be executed or not\n",
    "execute_option = opt_execute(option, state, environment)\n",
    "if execute_option:\n",
    "    next_state, cumulative_reward, timesteps_elapsed, done = multistep_opt_config(option, state, environment, gamma) # r(s,o) is cumulative reward for multi-step options\n",
    "    # One-step SMDP Q-Learning update since multi-step option\n",
    "    # Q(s,o) update for multi-step option\n",
    "    Q_values[state, option] += alpha*(cumulative_reward + (gamma**timesteps_elapsed)*(max(Q_values[next_state,:])-\n",
    "                                                                                        Q_values[state, option]))\n",
    "    state = next_state\n",
    "    episode_reward += cumulative_reward\n",
    "else: \n",
    "    state = next_state\n",
    "    done = False # done remains False since the state has not changed\n",
    "\n",
    "\n",
    "### CODE for multistep_opt function ###\n",
    "\n",
    "def multistep_opt_default(option, state, environment, gamma):\n",
    "    opt_done = False\n",
    "    goal_locations = {6:(0,0), 7:(0,4), 8:(4,0), 9:(4,3)} # 6,7,8,9 correspond to R,G,Y,B respectively\n",
    "    option_policy_mapping = {6: Opt_R_policy, 7: Opt_G_policy, 8: Opt_Y_policy, 9: Opt_B_policy}\n",
    "    cumulative_reward = 0\n",
    "    timestep_count = 0\n",
    "    while not opt_done:\n",
    "        taxi_row, taxi_col, _, _ = list(environment.decode(state))\n",
    "        action = option_policy_mapping[option][taxi_row][taxi_col]\n",
    "        next_state, reward, done, _, _ = environment.step(action)\n",
    "        timestep_count += 1\n",
    "        cumulative_reward += (gamma**(timestep_count-1))*reward\n",
    "        state = next_state\n",
    "        taxi_row, taxi_col, _, _ = list(environment.decode(state))\n",
    "        if (taxi_row, taxi_col) == goal_locations[option]:\n",
    "            opt_done = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85389497",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The algorithm was run with the configuration given below. The reward curve and Q-table is also shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1979f54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c64df283",
   "metadata": {},
   "source": [
    "## Description of the policy learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f725d8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "044d5bcf",
   "metadata": {},
   "source": [
    "# Intra-Option Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc98c2",
   "metadata": {},
   "source": [
    "## Code Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f51f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "040a3034",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a428429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e1263b",
   "metadata": {},
   "source": [
    "## Description of the policy learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba349b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d966435",
   "metadata": {},
   "source": [
    "# Comparison with mutually exclusive set of options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5af0d",
   "metadata": {},
   "source": [
    "## Code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b59ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a70077",
   "metadata": {},
   "source": [
    "Compare the performance with other set of options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95d71f",
   "metadata": {},
   "source": [
    "# Comparison between SMDP Q-Learning & Intra-Option Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66936d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
